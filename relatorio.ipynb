{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Disponibilização dos arquivos\n",
    "Basicamente temos 2 arquivos (entrada e saída):\n",
    "- [ocurr_index.idx](https://drive.google.com/file/d/1T0QckLU3bm-c8CbzUltMS4F6uC1sKUpx/view?usp=sharing)\n",
    "- [data.json](https://drive.google.com/file/d/1VOt2sOf6eyC8EU3RTLmc3L6fbpm-Dice/view?usp=sharing)\n",
    "\n",
    "Para converter o arquivo *ocurr_index* em JSON fizemos o uso da biblioteca _json_ do python. O código para fazer isso pode ser conferido no arquivo [convert_index.py](https://github.com/thiagodff/ri-indexador/blob/main/convert_index.py)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussão \n",
    "A parte mais desafiadora foi entender a estrutura inicial do indexador. Em certas ocasiões tivemos divergências quanto a correta implementação dos métodos e isso levou a muito retrabalho e avanço lento. Por vezes não estava muito claro como proceder com os parâmetros e métodos auxiliares.\n",
    "\n",
    "# vantagem/desvantagem sobre os modelos comparativos \n",
    "Fazer a indexação totalmente pela memória principal poderia facilitar o acesso aos dados, mas pode exigir muito do hardware do computador e fica difícil prever o tamanho do espaço ocupado, pois não se sabe quantas palavras em documentos diferentes existem. Já o uso de memória secundária vem como um alívio para memória RAM, porém acessar os dados depois salvos no arquivo binário `.idx` terá um custo associado a lentidão do disco rígido.\n",
    "\n",
    "# O modelo adotado para a solução\n",
    "A solução adotada nesse indexador vem buscar um equilíbrio entre uso de memória principal e secundária. O foco era aproveitar a velocidade da principal e a grande capacidade de armazenamento da secundária, mas sempre intercalando entre elas.\n",
    "\n",
    "# Possíveis melhorias de eficiência e desempenho\n",
    "Para deixar o processo mais eficiente seria interessante indexar frases completas ao invés de palavras. Já para deixar diminuir o consumo de memória poderia ser feita uma codificação que abstraísse a informação salva como uma notação em hexadecimal.\n",
    "\n",
    "# Bibliotecas externas usadas\n",
    "Claro que houve uso de recurso externo já consolidado do acervo do Python para ajudar nessa tarefa. Ao todo são 6, sendo elas: _gc_, _nltk_, _bs4_, _string_, _pickle_ e _json_.\n",
    "\n",
    "## gc\n",
    "Também conhecido como _garbage colector_, essa biblioteca foi usada para desabilitar o coletor de lixo durante a geração dos arquivos de forma a otimizar o acesso na memória secundária.\n",
    "\n",
    "## pickle\n",
    "Usada para escrita nos arquivos `.idx`.\n",
    "\n",
    "## nltk\n",
    "Responsável por dividir o texto em tokens tendo `espace` como divisor. Além disso, trazer o SnowballStemmer para realizar o stem das palavras filtradas.\n",
    "\n",
    "## bs4\n",
    "Responsável por remover marcadores de HTML do meio do texto.\n",
    "\n",
    "## string\n",
    "Não menos importante, essa biblioteca tem o papel de apontar todos os símbolos de pontuação através do atributo `string.punctuation`. A partir dele, foi possível obter todos os sinais que seriam removidos do texto.\n",
    "\n",
    "# Técnica de streaming adotada\n",
    "\n",
    "A técnica de streaming se baseia em definir um tamanho máximo para o vetor de ocorrências que funcionará como um Buffer, e caso esse limite seja atingido as ocorrências são salvas num arquivo.\n",
    "\n",
    "Na classe `FileIndex` é definido esse limite e no método `add_index_occur` a ocorrência é salva e logo após é realizada a verificação se a lista atingiu o seu limite, caso o tenha atingido é chamado o método `save_tmp_occurences` que é responsável por salvar em arquivo o conteúdo da lista.\n",
    "\n",
    "```.py\n",
    "class FileIndex(Index):\n",
    "\tTMP_OCCURRENCES_LIMIT = 1000000\n",
    "\t# ...\n",
    "\tdef add_index_occur(self, entry_dic_index: TermFilePosition, doc_id: int, term_id: int, term_freq: int):\n",
    "\t\tself.lst_occurrences_tmp.append(TermOccurrence(doc_id, term_id, term_freq))\n",
    "\t\tif len(self.lst_occurrences_tmp) >= FileIndex.TMP_OCCURRENCES_LIMIT:\n",
    "\t\t\tself.save_tmp_occurrences()\n",
    "```\n",
    "\n",
    "Quando a lista de ocorrências chega no seu limite o `save_tmp_occurences` é responsável por verificar se já existe outro arquivo salvo, caso exista ele irá ler o seu primeiro termo e comparar com o primeiro termo da lista previamente ordenada, a ocorrência que estiver o menor `term_id` será salva no novo arquivo e o processo irá se repetir até que todas as ocorrências da lista e do arquivo estejam salvos, e assim, conseguimos criar um arquivo com todas as ocorrências da memória principal e secundário de forma ordenada.\n",
    "\n",
    "# Estrutura do índice\n",
    "\n",
    "O índice é representado pela classe `TermOccurence` que salva três termos, o identificador do documento nomeado como `doc_id`, o identificador do termo nomeado como `term_id` e a frequência do termo que é salvo em `term_freq`.\n",
    "\n",
    "```.py\n",
    "class TermOccurrence:\n",
    "    def __init__(self, doc_id: int, term_id: int, term_freq: int):\n",
    "        self.doc_id = doc_id\n",
    "        self.term_id = term_id\n",
    "        self.term_freq = term_freq\n",
    "```\n",
    "\n",
    "Ao recuperar os dados do índice, recebos o sequinte padrão: `(term_id: 1, doc_id: 0, term_freq: 3)`\n",
    "\n",
    "# Resultado obtido\n",
    "Foram feitos dois testes de desempenho no em `TP2 - Dojo - Indice.ipynb`, no primeiro, os valores indexados não foram armazenados em arquivos. O teste inteiro ficou a cargo da memória principal. Para 2500 documentos e 500 termos obtidos por documento, foram obtidos os seguintes resultados: \n",
    "\n",
    "> Máximo 203.424927 MB de Memória RAM usada.\n",
    "\n",
    "> Indexou 1.250.000 ocorrências em 13.440106 segundos.\n",
    "\n",
    "Já no segundo teste, foi usado armazenamento em memória secundária. Sendo os mesmos 2500 documentos e 500 termos obtidos por documento, porém com limite de 100.000 termos na memória principal. Assim foram obtidos os seguintes resultados:\n",
    "\n",
    "> Máximo 20.528784 MB de Memória RAM usada.\n",
    "\n",
    "> Indexou 1.250.000 ocorrências em 1976.981396 segundos (33 minutos aprox.).\n",
    "\n",
    "> Foram gerados 12 arquivos `.idx`.\n",
    "\n",
    "> 164.748450 segundos para cada arquivo (3 minutos aprox.)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}