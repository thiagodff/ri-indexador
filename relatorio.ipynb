{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Disponibilização dos arquivos\r\n",
    "**Após a criação o índice, ele deve ser gravado em arquivo (inclusive o vocabulário). Favor disponibilizar o link para que eu possa fazer o download do arquivo (via dropbox, por ex). Além de instruções completas para que eu possa abrí­lo. A biblioteca json pode ajudar.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Discussão \n",
    "A parte mais desafiadora foi entender a estrutura inicial do indexador. Em certas ocasiões tivemos divergências quanto a correta implementação dos métodos e isso levou a muito retrabalho e avanço lento. Por vezes não estava muito claro como proceder com os parâmetros e métodos auxiliares.\n",
    "\n",
    "# vantagem/desvantagem sobre os modelos comparativos \n",
    "Fazer a indexação totalmente pela memória principal poderia facilitar o acesso aos dados, mas pode exigir muito do hardware do computador e fica difícil prever o tamanho do espaço ocupado pois não se sabe quantas palavras em documentos diferentes existem. Já o uso de memória secundária vem como um alívio para memória RAM, porém acessar os dados depois salvos no arquivo binário `.idx` terá um custo associado a lentidão do disco rígido.\n",
    "\n",
    "# O modelo adotado para a solução\n",
    "A solução adotada nesse indexador vem buscar um equilíbrio entre uso de memória princípal e secundária. O foco era aproveitar a velocidade da principal e a grande capacidade de armazenamento da secundária, mas sempre intercalando entre elas.\n",
    "\n",
    "# Possíveis melhorias de eficiência e desempenho\n",
    "Para deixar o processo mais eficiente seria interessante indexar frases completas ao invés de palavras. Já para deixar diminuir o consumo de memória poderia ser feita uma codificação que abstraísse a informação salva como uma notação em hexadecimal.\n",
    "\n",
    "# Bibliotecas externas usadas\n",
    "Claro que houve uso de recurso externo já consolidado do acervo do Python para ajudar nessa tarefa. Ao todo são 5, sendo elas: gc, pickle, nltk, bs4 e string\n",
    "\n",
    "## gc\n",
    "Também conhecido como garbage colector, essa biblioteca foi usada para desabilitar o coletor de lixo durante a geração dos arquivos de forma a otimizar o acesso na memória secundária.\n",
    "\n",
    "## pickle\n",
    "Usada para escrita nos arquivos `.idx`.\n",
    "\n",
    "## nltk\n",
    "Responsável por dividir o texto em tokens tendo `espace` como divisor. Além disso trazer o SnowballStemmer para realizar o stem das palavras filtradas.\n",
    "\n",
    "## bs4\n",
    "Responsável por remover marcadores de HTML do meio do texto.\n",
    "\n",
    "## string\n",
    "Não menos importante, essa biblioteca tem o papel de apontar todos os símbolos de pontuação através do atributo string.punctuation. A partir dele, foi possível obter todos os sinais que seriam removidos do texto.\n",
    "\n",
    "# Técnica de streaming adotada\n",
    "\n",
    "A técnica de streaming se baseia em definir um tamanho máximo para o vetor de ocorrências que funcionará como um Buffer, e caso esse limite seja atingido as ocorrências são salvas em um arquivo.\n",
    "\n",
    "Na classe `FileIndex` é definido esse limite e no método `add_index_occur` a ocorrência é salva e logo após é realizada a verificação se a lista atingiu seu limite, caso o tenha atingido é chamado o método `save_tmp_occurences` que é resposável por salvar em arquivo o conteúdo da lista.\n",
    "\n",
    "```python\n",
    "class FileIndex(Index):\n",
    "\tTMP_OCCURRENCES_LIMIT = 1000000\n",
    "\t# ...\n",
    "\tdef add_index_occur(self, entry_dic_index: TermFilePosition, doc_id: int, term_id: int, term_freq: int):\n",
    "\t\tself.lst_occurrences_tmp.append(TermOccurrence(doc_id, term_id, term_freq))\n",
    "\t\tif len(self.lst_occurrences_tmp) >= FileIndex.TMP_OCCURRENCES_LIMIT:\n",
    "\t\t\tself.save_tmp_occurrences()\n",
    "```\n",
    "\n",
    "Quando a lista de ocorrências chega em seu limite o `save_tmp_occurences` é responssável por verificar se já existe outro arquivo salvo, caso exista ele irá ler seu primeiro termo e comparar com o primeiro termo da lista previamente ordenada, a ocorrência que estiver o menor `term_id` será salva no novo arquivo e o processo irá se repetir até que todas as ocorrências da lista e do arquivo estejam salvos, e assim, conseguimos criar um novo arquivo com todas as ocorrências da memória principal e secundário de forma ordenada.\n",
    "\n",
    "# Estrutura utilizada    \n",
    "**Qual foi a estrutura do índice adotada?** \n",
    "\n",
    "# Resultado obtido    \n",
    "Foram feitos dois testes de desempenho no em `TP2 - Dojo - Indice.ipynb`, no primeiro, os valores indexados não foram armazenados em arquivos. O teste inteiro ficou a cargo da memória principal. Para 2500 documentos e 500 termos obtidos por documento, foram obtidos os seguintes resultados: \n",
    "\n",
    "> Máximo 203.424927 MB de Memória RAM usada.\n",
    "\n",
    "> Indexou 1.250.000 ocorrências em 13.440106 segundos.\n",
    "\n",
    "Já no segundo teste, foi usado armazenamento em memória secundária. Sendo os mesmos 2500 documentos e 500 termos obtidos por documento, porém com limite de 100.000 termos na memória principal. Assim foram obtidos os seguintes resultados:\n",
    "\n",
    "> Máximo 20.528784 MB de Memória RAM usada.\n",
    "\n",
    "> Indexou 1.250.000 ocorrências em 1976.981396 segundos (33 minutos aprox.).\n",
    "\n",
    "> Foram gerados 12 arquivos `.idx`.\n",
    "\n",
    "> 164.748450 segundos para cada arquivo (3 minutos aprox.)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}